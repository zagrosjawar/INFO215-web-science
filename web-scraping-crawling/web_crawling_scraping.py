# a. Start crawling from https://en.wikipedia.org/wiki/Web_scraping# b. Using BeautifulSoup's find_all() function get all the links from 'See also' section# c. for each link do the following# c.1 Load selected article page# c.2. Print out selected articles first paragraph.# basic error handling.from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom urllib.error import URLErrorfrom bs4 import BeautifulSoup# start crawling# error handling# getting linksurl = "https://en.wikipedia.org/wiki/Web_scraping"def get_links(url):    try:        html = urlopen(url)    except HTTPError as e:        print(e)    except URLError as e:        print(e)    else:        soup = BeautifulSoup(html, "html.parser")    try:        links = soup.find("div", {"class": "div-col"}).find("ul").find_all("a")    except AttributeError as e:        print(e)    return links# finding first paragraphdef get_first_par(article):    html = urlopen("https://en.wikipedia.org{}".format(article))    soup = BeautifulSoup(html, "html.parser")    first_par = soup.find("p")    # to move to next paragraph if the first paragraph is empty but it could be more refined    while len(first_par.text.strip()) == 0:        first_par = first_par.find_next_sibling("p")    return first_par.textfor link in get_links(url):    print(get_first_par(link["href"]))