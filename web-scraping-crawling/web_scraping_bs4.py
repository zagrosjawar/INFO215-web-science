# extracting all links (internal and external links)# extracting all images src attributes and printing out absolute link reference.# from: https://en.wikipedia.org/wiki/Star_Wars:_The_Rise_of_Skywalker# XPATH expressionsfrom urllib.request import urlopenfrom bs4 import BeautifulSoupfrom urllib.parse import urljoinimport reurl = "https://en.wikipedia.org/wiki/Star_Wars:_The_Rise_of_Skywalker"base_url = "https://en.wikipedia.org"base_url2 = "https://upload.wikimedia.org"# getting HTMLhtml = urlopen(url)# beautiful soup objectbs = BeautifulSoup(html.read(), "html.parser")# extracting all linkslinks = bs.find_all("a")print(type(links))def find_links(links):    for link in links:        try:            href = link['href']            if href is not None:                if href.startswith("http"):                    print(href)                else:                    abs_url = urljoin(base_url, href)                    print(abs_url)        except KeyError:            href = Nonefind_links(links)# 1.2 extract all images src attribute from the page. You must print out absolute link reference.images = bs.find_all("img", src=re.compile(r"^(//upload)(?!.*\.svg).*$"))def find_src_link(imgs):    for img in imgs:        try:            src_link = img['src']            abs_src_link = urljoin(base_url2, src_link)        except KeyError:            src = None        print(abs_src_link)print()print("img src absolute link reference")find_src_link(images)